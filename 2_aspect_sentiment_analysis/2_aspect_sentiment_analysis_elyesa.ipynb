{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6a50e3-5404-41bd-9678-f37d72d8c572",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Aspect Based Sentiment Analysis (ABSA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623aa282-2a7e-4e8d-9cfe-470751dfa5aa",
   "metadata": {},
   "source": [
    "source: https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-a-practical-approach-8f51029bbc4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1e5f63-c769-45ce-b3c7-ffbd6fdb2cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install nltk\n",
    "#!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71bf7e3-043e-4c14-bcf2-d29f2effe2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14296fb-5264-4535-898f-44416960ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/elyesaseidel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/elyesaseidel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/elyesaseidel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the Stanford English model and some nltk tools\n",
    "#stanfordnlp.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb78ab0-2817-469d-bda2-e063202319b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample text review on which we will perform ABSA\n",
    "txt = \"The Sound Quality is great but the battery life is very bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d3a820-5195-46f1-b8f0-d07f22a5db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LowerCase the text and tokenize the sentence\n",
    "txt = txt.lower()\n",
    "sentList = nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e0a0d6-e5d5-4097-ad58-475a710ebca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('sound', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('but', 'CC'), ('the', 'DT'), ('battery', 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('bad', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# for each sentence in the <sentList> tokenize it and perform POS tagging and store it into a tagged list\n",
    "for line in sentList:\n",
    "    txt_list = nltk.word_tokenize(line)\n",
    "    taggedList = nltk.pos_tag(txt_list)\n",
    "print(taggedList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9567e6-5d8a-44c4-b47d-5f559b0c6439",
   "metadata": {},
   "source": [
    "**There are many instances where a feature is represented by multiple words so we need to handle that first by joining multiple words features into a one-word feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74be1d8d-721b-40e2-a9e1-7d09997a6aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the soundquality is great but the batterylife is very bad .\n"
     ]
    }
   ],
   "source": [
    "# join multiple words features into one-word feature\n",
    "newwordList = []\n",
    "flag = 0\n",
    "for i in range(0,len(taggedList)-1):\n",
    "    if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "        newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "        flag=1\n",
    "    else:\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "        newwordList.append(taggedList[i][0])\n",
    "        if(i==len(taggedList)-2):\n",
    "            newwordList.append(taggedList[i+1][0])\n",
    "finaltxt = ' '.join(word for word in newwordList)\n",
    "print(finaltxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a652e03e-6e0f-46f1-a9ca-07ac3f46c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and POS tag the new sentence\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "taggedList = nltk.pos_tag(wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea6e6c-b615-4fc7-9638-ae2d90b57214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/elyesaseidel/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/elyesaseidel/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/elyesaseidel/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n"
     ]
    }
   ],
   "source": [
    "# use the Stanford NLP Dependency Parser to get the relations between each word\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "doc = nlp(finaltxt)\n",
    "dep_node = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "for i in range(0, len(dep_node)):\n",
    "    if (int(dep_node[i][1]) != 0):\n",
    "        dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "print(dep_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3a20a-c9c0-4b78-8ce3-774492367cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only those sublists from the <dep_node> that could probably contain the features\n",
    "featureList = []\n",
    "categories = []\n",
    "for i in taggedList:\n",
    "    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "        featureList.append(list(i))\n",
    "        totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "        categories.append(i[0])\n",
    "print(featureList)\n",
    "print(categoriesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ecc89-1a11-44f1-89cb-3dcf8a4c849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine to which of the words these features in the feature list are related to (using the <dep_node> list and the <featureList>) \n",
    "fcluster = []\n",
    "for i in featureList:\n",
    "    filist = []\n",
    "    for j in dep_node:\n",
    "        if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "            if(j[0]==i[0]):\n",
    "                filist.append(j[1])\n",
    "            else:\n",
    "                filist.append(j[0])\n",
    "    fcluster.append([i[0], filist])\n",
    "print(fcluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd315a1-cdfa-4100-9b75-fcf2c3183d94",
   "metadata": {},
   "source": [
    "**So as you can see we have got the feature words and for each word a list of words it is related to.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014b22e-425a-4397-bb5d-0792828adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the feature Nouns List from the <fcluster>\n",
    "finalcluster = []\n",
    "dic = {}\n",
    "for i in featureList:\n",
    "    dic[i[0]] = i[1]\n",
    "for i in fcluster:\n",
    "    if(dic[i[0]]==\"NN\"):\n",
    "        finalcluster.append(i)\n",
    "print(finalcluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24755721-d3e2-433c-8a52-f7ef2865d2d4",
   "metadata": {},
   "source": [
    "**So with this, we have got the list of the features and their respective sentiment words within a sentence, now all you have to do is to check whether the sentiment word is positive, negative or neutral.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c726b8-550b-4585-ad23-74578c097494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc65ee8-ecfc-4ce8-8fe5-e7af19a61de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full code \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp\n",
    "\n",
    "# Make sure you have downloaded the StanfordNLP English model and other essential tools using,\n",
    "# stanfordnlp.download('en')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def aspect_sentiment_analysis(txt, stop_words, nlp):\n",
    "    \n",
    "    txt = txt.lower() # LowerCasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        newtaggedList = []\n",
    "        txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "        taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(taggedList)-1):\n",
    "            if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i==len(taggedList)-2):\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in newwordList) \n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "\n",
    "        doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine\n",
    "        \n",
    "        # Getting the dependency relations betwwen the words\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "\n",
    "        # Coverting it into appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "        featureList = []\n",
    "        categories = []\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i)) # For features for each sentence\n",
    "                totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\n",
    "                categories.append(i[0])\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "            \n",
    "    for i in totalfeatureList:\n",
    "        dic[i[0]] = i[1]\n",
    "    \n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            finalcluster.append(i)\n",
    "        \n",
    "    return(finalcluster)\n",
    "\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"The Sound Quality is great but the battery life is very bad.\"\n",
    "\n",
    "print(aspect_sentiment_analysis(txt, stop_words, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95189490-0b69-4df8-b18e-94d3ee4681fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
